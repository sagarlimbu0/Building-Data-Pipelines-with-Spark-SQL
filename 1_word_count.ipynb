{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6ae0e2-38d5-4976-8343-851e91527a04",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "- word count from the text file using `pyspark` to create RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439d04f6-d442-4903-ab51-7d5da8397589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting tweepy\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /opt/conda/lib/python3.11/site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /opt/conda/lib/python3.11/site-packages (from tweepy) (2.31.0)\n",
      "Collecting requests-oauthlib<2,>=1.2.0 (from tweepy)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.4.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27.0->tweepy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27.0->tweepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.27.0->tweepy) (2023.7.22)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.4.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: regex, requests-oauthlib, nltk, tweepy, textblob\n",
      "Successfully installed nltk-3.8.1 regex-2024.4.16 requests-oauthlib-1.3.1 textblob-0.18.0.post0 tweepy-4.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob tweepy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7db5757-ddc0-47a7-8d68-00ee110ca6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "from textblob.utils import strip_punc\n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf358ace-5995-4b0c-b436-9eff077c056e",
   "metadata": {},
   "source": [
    "### Create Configurations for pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164cc6c3-6017-4892-ad9b-749b11d1470f",
   "metadata": {},
   "source": [
    "## using cluster; we use `cluster URL`\n",
    "- here, we use local[*] that will specifiy to use all cluster available to execute the thread\n",
    "- EG: with local computer 4 core, it uses 4 executor to worker nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7407d837-84be-4ce2-a61a-c5333af5eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_url = 'local[*]'\n",
    "configuration = SparkConf()\\\n",
    "    .setAppName('RomeoJulietCounter')\\\n",
    "    .setMaster(cluster_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f34938f5-4e98-41db-ae37-7c0eb8c78f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the configurations\n",
    "sc= SparkContext(conf= configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51309b-851a-4e7f-b619-f675606f62bd",
   "metadata": {},
   "source": [
    "### Reading text file with pyspark\n",
    "- Create pipeline to transform the data and count the total counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab79682e-9cd0-4433-b4ba-e2642e400840",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized= sc.textFile('Romeo and Juliet.txt')\\\n",
    "            .flatMap(lambda line: line.lower().split())\\\n",
    "            .map(lambda wd: strip_punc(wd, all= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6988b-aded-4448-b198-948c96546399",
   "metadata": {},
   "source": [
    "### Filter the Words by excluding STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2641f0ee-990b-4067-b719-5bf4a4cf7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de8cbffe-79a9-41e8-ae06-cf43bdf535d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of stop words\n",
    "stop_words= open(\"english.txt\", \"r\")\n",
    "stop_words = stop_words.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97a8ca8-7657-40b1-86a7-c082f2148dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter the words\n",
    "filtered = tokenized.filter(lambda word: word not in stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791872fd-3d76-4039-8d8c-6f912ed4eda7",
   "metadata": {},
   "source": [
    "### Filter RDD by mapReduce\n",
    "- Reduce by key to count the total unique words by using `add` function``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37491c80-26e8-4a10-b9bf-77c8a483561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creates a tuple for every unique word\n",
    "## second, reduce by key to count by unique words\n",
    "word_counts = filtered.map(lambda wd: (wd, 1))\\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33697c-fa86-4b44-9a87-1f8422127c5f",
   "metadata": {},
   "source": [
    "### Filter by second index\n",
    "- `Total Counts` for each unqiue words\n",
    "- filter the words that has length less than 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa13633c-e84a-415d-ae6d-bea4e6bed0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_counts= word_counts.filter(lambda wd: wd[1] >=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b754c003-2bd3-43f6-9ef1-83763bd1dc80",
   "metadata": {},
   "source": [
    "### Sorting by Descending order\n",
    "- using `collect()` method will trigger all assigned methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3b6014-3d56-4e93-ba61-71ed3e4e407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items = sorted(filtered_counts.collect(), key= itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b509e5-b9a5-45a2-aaea-8112f547eedd",
   "metadata": {},
   "source": [
    "### Dispaly the sorted count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c9823c8-aa5e-482d-be1a-d192c7ad456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou 277\n",
      "thy 164\n",
      "rom 163\n",
      "nurse 150\n",
      "romeo 143\n",
      "thee 138\n",
      "love 137\n",
      "jul 117\n",
      "shall 112\n",
      "come 99\n",
      "friar 92\n",
      "project 89\n",
      "ill 84\n",
      "enter 82\n",
      "good 82\n",
      "go 76\n",
      "man 72\n",
      "well 69\n",
      "death 69\n",
      "lady 68\n",
      "night 68\n",
      "juliet 65\n",
      "may 65\n",
      "ben 64\n",
      "hath 64\n",
      "one 62\n",
      "mer 62\n"
     ]
    }
   ],
   "source": [
    "max_len = max([ len(word) for word, count in sorted_items])\n",
    "for word, count in sorted_items:\n",
    "    #print(f'{word > {max_len}} : {count}')\n",
    "    print(word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c54a50-ef21-47cc-8995-f141752b5175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa4ade-e1b1-4e55-abc2-1c909c9c2b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
