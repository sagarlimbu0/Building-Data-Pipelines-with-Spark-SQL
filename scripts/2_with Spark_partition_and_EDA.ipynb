{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8227185-cfca-436e-b805-1a7131eb1d28",
   "metadata": {},
   "source": [
    "#### Objective:\n",
    "- use the kernel from `Anaconda` to use `pyspark` package\n",
    "- EDA with spark on compressed file\n",
    "\n",
    "- Repartitioning the spark dataframe:\n",
    "    - If plan to write this DataFrame to disk, especially as Parquet files, repartitioning can help control the number and size of output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e4d0d3-2d2d-4ab1-b575-1475053031c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "## schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2116974-918f-43c3-a4f7-5aae7691576c",
   "metadata": {},
   "source": [
    "* install parquet library for pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65988e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a975c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121a8bf2-53c1-4f9b-8192-d375bfd84c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1783ec-7810-42a2-b2bd-8491607e0ac8",
   "metadata": {},
   "source": [
    "#### Create Spark context with `SparkConf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2be0ad-a272-48ea-98c0-ce6aa327e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_url= 'local[*]'\n",
    "# create config file\n",
    "config = SparkConf()\\\n",
    "    .setAppName('readAWSCrawlData')\\\n",
    "    .setMaster(cluster_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45bde787-1b90-4aa7-8e62-cf1dd75145df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b876b90-bbb2-424f-9de6-14b89944a5a5",
   "metadata": {},
   "source": [
    "## 2. Another way of creating Spark Session\n",
    "\n",
    "=>\n",
    "    config= SparkConf()\\\n",
    "        .setAppName('readAWSCrawlData')\\\n",
    "        .setMaster('local[*]')\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf= config)\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915cf4f-9261-4d85-8b11-bd0191b97b71",
   "metadata": {},
   "source": [
    "### Read the compressed file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a2094f5-57b6-495c-92f6-87c27d857c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_aws_zipped= sc.read\\\n",
    "#     .format(\"csv\")\\\n",
    "#     .option(\"compression\", \"gzip\")\\\n",
    "#     .option(\"header\", True)\\\n",
    "#     .load(\"data_sources/AWS_web_crawl/cdx-00005.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acad62-9124-43fc-9f1d-e7ae803ef5ff",
   "metadata": {},
   "source": [
    "### Read with spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a39625-7a1a-47e9-aba6-7661a66c0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config= SparkConf()\\\n",
    "    .setAppName('readAWSCrawlData')\\\n",
    "    .setMaster('local[*]')\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf= config)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84f84c-1fea-4aac-89e5-2809bbb69cfc",
   "metadata": {},
   "source": [
    "#### Path to GZIP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601abe28-c635-43c6-b2cd-6343295ca31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"data/CC-MAIN-2024-33/cdx-00001.gz\"\n",
    "\n",
    "path_2= \"data/CC-MAIN-2024-33/cdx-00008.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91894eeb-fa65-4ec0-a7d4-2e64ad8d06eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zipped= spark.read\\\n",
    "#     .format(\"csv\")\\\n",
    "#     .option(\"compression\", \"gzip\")\\\n",
    "#     .option(\"header\", False)\\\n",
    "#     .load(\"data_sources/AWS_web_crawl/cdx-00005.gz\")\n",
    "\n",
    "\n",
    "df_zipped= spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"compression\", \"gzip\")\\\n",
    "    .option(\"header\", False)\\\n",
    "    .load(path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc9e8ce-9dd8-4339-9119-d29c77c5c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zipped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc2b8308-d264-4a8a-afe5-aded8235cf41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zipped.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2184cfe4-e038-49df-9ec4-2e720312cd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+----------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|_c0|                 _c1|                 _c2|                 _c3|             _c4|                 _c5|             _c6|                 _c7|                 _c8|                 _c9|\n",
      "+---+--------------------+--------------------+--------------------+----------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "| be|trois-frontieres)...| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"302\"| \"digest\": \"ZO5BO...| \"length\": \"695\"| \"offset\": \"26539...| \"filename\": \"cra...| \"redirect\": \"htt...|\n",
      "| be|trois-frontieres)...| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"302\"| \"digest\": \"ZO5BO...| \"length\": \"690\"| \"offset\": \"25627...| \"filename\": \"cra...| \"redirect\": \"htt...|\n",
      "+---+--------------------+--------------------+--------------------+----------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zipped.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bab19b-4f3c-4a63-ade0-c1d29926823b",
   "metadata": {},
   "source": [
    "## Change the Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee9e2b86-e1dd-4011-8033-824e0b2cfcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## shcema\n",
    "df_zipped_ = df_zipped\\\n",
    "    .withColumn(\"c1_\", F.col(\"_c0\").cast(StringType())).drop(\"_c0\")\\\n",
    "    .withColumn(\"c2_\", F.col(\"_c1\").cast(StringType())).drop(\"_c1\")\\\n",
    "    .withColumn(\"url\", F.col(\"_c2\").cast(StringType())).drop(\"_c2\")\\\n",
    "    .withColumn(\"mime\", F.col(\"_c3\").cast(StringType())).drop(\"_c3\")\\\n",
    "    .withColumn(\"mime_type\", F.col(\"_c4\").cast(StringType())).drop(\"_c4\")\\\n",
    "    .withColumn(\"status\", F.col(\"_c5\").cast(StringType())).drop(\"_c5\")\\\n",
    "    .withColumn(\"digest_offset\", F.col(\"_c6\").cast(StringType())).drop(\"_c6\")\\\n",
    "    .withColumn(\"length\", F.col(\"_c7\").cast(StringType())).drop(\"_c7\")\\\n",
    "    .withColumn(\"offset\", F.col(\"_c8\").cast(StringType())).drop(\"_c8\")\\\n",
    "    .withColumn(\"filename\", F.col(\"_c9\").cast(StringType())).drop(\"_c9\")\\\n",
    "    # .withColumn(\"content_charset\", F.col(\"_c10\").cast(StringType())).drop(\"_c10\")\\\n",
    "    # .withColumn(\"language\", F.col(\"_c11\").cast(StringType())).drop(\"_c11\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53887633-1a9d-48da-bddb-a44ce2bfeccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+----------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|c1_|                 c2_|                 url|                mime|       mime_type|              status|   digest_offset|              length|              offset|            filename|\n",
      "+---+--------------------+--------------------+--------------------+----------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "| be|trois-frontieres)...| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"302\"| \"digest\": \"ZO5BO...| \"length\": \"695\"| \"offset\": \"26539...| \"filename\": \"cra...| \"redirect\": \"htt...|\n",
      "| be|trois-frontieres)...| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"302\"| \"digest\": \"ZO5BO...| \"length\": \"690\"| \"offset\": \"25627...| \"filename\": \"cra...| \"redirect\": \"htt...|\n",
      "+---+--------------------+--------------------+--------------------+----------------+--------------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zipped_.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b130212-a192-4e74-9aa3-b4f44523aea0",
   "metadata": {},
   "source": [
    "##### Save as Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71306d46-57ba-49c4-a238-24e63354ab29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zipped_.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d451b807-6c4b-496f-a277-3f3f28c91b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zipped_=df_zipped_.repartition(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd28089-370a-4b04-9ec2-28ae99b06257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zipped_.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6ddbe-2c6e-41ac-9598-ee668ca1c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_zipped_.write.parquet(\"2024_00008.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaaf714-b5e0-4112-8825-98d311baaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_zipped_.write.mode(\"overwrite\").parquet(\"data/2024_00008.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "975fc22c-a7af-4506-b48d-8d1a49cb70c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='be', _c1='trois-frontieres)/f/n/plans_cartes.php 20240812104747 {\"url\": \"https://www.trois-frontieres.be/F/N/plans_cartes.php\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/n/sources.php 20240812110044 {\"url\": \"https://www.trois-frontieres.be/F/N/sources.php\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/n/trains.php 20240812112818 {\"url\": \"https://www.trois-frontieres.be/F/N/trains.php\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/n/viaduc.php 20240812100754 {\"url\": \"https://www.trois-frontieres.be/F/N/viaduc.php\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/n/welkenraedt_tout_savoir.php 20240812103816 {\"url\": \"https://www.trois-frontieres.be/F/N/welkenraedt_tout_savoir.php\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/nom_fam_prof.php 20240812004417 {\"url\": \"https://www.trois-frontieres.be/F/nom_fam_prof.php\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=eynatten%204731 20240812095325 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=EYNATTEN%204731\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=fouron%20le%20comte%203798 20240812094731 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=FOURON%20LE%20COMTE%203798\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=fouron%20st%20martin%203790 20240812094347 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=FOURON%20ST%20MARTIN%203790\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=fouron%20st%20pierre%203792 20240812102419 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=FOURON%20St%20PIERRE%203792\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=gemmenich%204851 20240812113450 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=GEMMENICH%204851\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=hauset%204730 20240812102352 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=HAUSET%204730\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=henri-chapelle%204841 20240812113353 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=HENRI-CHAPELLE%204841\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=kettenis%204701 20240812103332 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=KETTENIS%204701\"', _c2=' \"mime\": \"text/html\"'),\n",
       " Row(_c0='be', _c1='trois-frontieres)/f/pensions_bb.php?search=la%20calamine%204720 20240812113016 {\"url\": \"https://www.trois-frontieres.be/F/pensions_BB.php?search=LA%20CALAMINE%204720\"', _c2=' \"mime\": \"text/html\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zipped.select(df_zipped.columns[:3]).take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "072cb0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9858933"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zipped_.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7aa878-f756-422d-a7c1-5b7339bf170d",
   "metadata": {},
   "source": [
    "### Partition\n",
    "- To allow all the spark executors to process on sub-folders, rather handling one single Large file\n",
    "- It is a lazy function\n",
    "\n",
    "### NOTE: parition number\n",
    "- For 4 Cores, starting with 16-32 paritions and adjusting based on the actual performance of `Spark Jobs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e06adb8b-dce8-40d1-9e16-41794a70f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zipped= df_zipped.repartition(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e55b57-bca3-4580-90d0-176be1b02b82",
   "metadata": {},
   "source": [
    "#### Write the partitioned file to dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01493a4f-80bc-490e-9643-ffb3ad6c7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df_zipped.write.parquet(\"data_sources/aws_crawl_00005/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df9dce4f-c2c2-44fd-a057-43f54043ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zipped.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5708b-82b3-43ac-a72b-39e32389ab56",
   "metadata": {},
   "source": [
    "### Perform Substring of the URL column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b3717-1e51-4b46-ab8f-d0037377f96e",
   "metadata": {},
   "source": [
    "#### Custom function\n",
    "- user-defined function to split the column `url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef2f3bed-8a94-4e00-ad52-9c9950295e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## funciton to perform substring on Full URL path\n",
    "def process_url(df):\n",
    "\n",
    "    ## replace the NaN with NONE value\n",
    "    # df= df.dropna()\n",
    "\n",
    "    #df[\"url\"]= df[\"url\"].replace({ np.nan: 0})\n",
    "\n",
    "    ## list of substrings\n",
    "    ip_path= []\n",
    "    resource_path= []\n",
    "    time_stamp = []\n",
    "    url= []\n",
    "\n",
    "    \n",
    "    # iterate over each row\n",
    "    for j in range(0, len(df)):\n",
    "\n",
    "#        if pd.notna(df[\"url\"][j]):\n",
    "\n",
    "        # split the url\n",
    "        full_url= df[\"url\"][j].split(' ')        \n",
    "            \n",
    "        # Get full path\n",
    "        ip_resource= full_url[0].split(\")\")\n",
    "        \n",
    "        # get IP path\n",
    "        ip_path.append(ip_resource[0])\n",
    "        # Get Resource path\n",
    "        resource_path.append(ip_resource[1])\n",
    "    \n",
    "        # get timestamp\n",
    "        time_stamp.append(full_url[1])\n",
    "        # Get the full URL\n",
    "        url.append(full_url[3].replace('\"', ''))\n",
    "\n",
    "            \n",
    "    df_new= df.copy()\n",
    "\n",
    "    # Append the values\n",
    "    df_new[\"Ip_path\"]= ip_path\n",
    "    df_new[\"Resource_path\"]= resource_path\n",
    "    df_new[\"TimeStamp\"]= time_stamp\n",
    "    df_new[\"URL\"] = url\n",
    "\n",
    "    ## drop URL column\n",
    "    df_new= df_new.drop(columns= [\"url\"])\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e5736-9119-4fee-b661-d3a4ec999975",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "Partitioning data in Apache Spark is a fundamental concept that plays a crucial role in the performance and efficiency of your Spark applications. Here’s a detailed explanation of what partitioning does and why it is important:\n",
    "\n",
    "### What Partitioning Does\n",
    "\n",
    "Partitioning in Spark refers to dividing a large dataset into smaller chunks, or partitions, which can be processed in parallel across the nodes of a cluster. Each partition is a logical division of the data that allows Spark to distribute and process the data concurrently.\n",
    "\n",
    "### Importance of Partitioning\n",
    "\n",
    "1. **Parallelism**:\n",
    "   - **Improved Parallel Processing**: Partitioning enables parallel processing by allowing multiple tasks to work on different parts of the data simultaneously. This can significantly reduce the overall processing time.\n",
    "   - **Optimal Resource Utilization**: By distributing tasks across available cores and nodes, Spark can better utilize the cluster's computational resources.\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - **Data Locality**: Partitions can be processed on the node where the data resides, reducing the need for data transfer over the network and thus enhancing performance.\n",
    "   - **Balanced Workload**: Proper partitioning ensures that the workload is evenly distributed across the cluster, preventing some nodes from being overloaded while others are idle.\n",
    "\n",
    "3. **Memory Management**:\n",
    "   - **Avoiding Out of Memory Errors**: Smaller partitions are easier to fit into memory, reducing the likelihood of out-of-memory errors and enabling more efficient garbage collection.\n",
    "\n",
    "4. **Fault Tolerance**:\n",
    "   - **Resilience to Failures**: If a node fails, only the tasks related to the partitions on that node need to be re-executed, rather than the entire job. This makes Spark applications more resilient to hardware failures.\n",
    "\n",
    "5. **Scalability**:\n",
    "   - **Handling Large Datasets**: Partitioning allows Spark to handle large datasets by breaking them into manageable chunks. This scalability is essential for big data processing.\n",
    "\n",
    "### How Partitioning Works in Spark\n",
    "\n",
    "- **Default Partitioning**: When you create an RDD or DataFrame, Spark automatically partitions it based on default settings or the source data's natural partitions (e.g., HDFS blocks).\n",
    "- **Custom Partitioning**: You can explicitly control the number of partitions using operations like `repartition`, `coalesce`, and during data loading with options like `numPartitions`.\n",
    "\n",
    "### Key Partitioning Operations\n",
    "\n",
    "1. **repartition**:\n",
    "   - **Description**: Increases or decreases the number of partitions and involves a full shuffle of the data, redistributing it across the nodes.\n",
    "   - **Use Case**: When you need to increase parallelism or balance the data more evenly before a shuffle-intensive operation.\n",
    "\n",
    "2. **coalesce**:\n",
    "   - **Description**: Reduces the number of partitions without a full shuffle, typically by merging partitions within the same node.\n",
    "   - **Use Case**: When you want to decrease the number of partitions for efficiency, often used after filtering operations to reduce the number of partitions.\n",
    "\n",
    "3. **partitionBy** (for DataFrames):\n",
    "   - **Description**: Specifies a column to partition the data by when writing it out to a file system.\n",
    "   - **Use Case**: Optimizes data layout for future reads, especially for queries that filter by the partitioned column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350c2e0-c2fd-4ac4-a375-552205821454",
   "metadata": {},
   "source": [
    "### Read the Parition the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7b736-7f4f-4c93-a2c0-53e7601220db",
   "metadata": {},
   "source": [
    "### Schema\n",
    "- assign column names when reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "053aa4a0-07f1-4b34-b150-c454c6aca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign column name\n",
    "new_col_names= [\"c1_\", \"c2_\", \"url\", \"mime\", \"mime_type\", \"status\", \"digest_offset\", \"length\", \"offset\", \"filename\",\n",
    "                \"content_charset\", \"language\"]\n",
    "\n",
    "# ## shcema\n",
    "# schema_ = StructType([\n",
    "#     StructField(\"c1_\", StringType(), True),\\\n",
    "#     StructField(\"c2_\", StringType(), True),\\\n",
    "#     StructField(\"url\", StringType(), True), \\\n",
    "#     StructField(\"mime\", StringType(), True), \\\n",
    "#     StructField(\"mime_type\", StringType(), True), \\\n",
    "#     StructField(\"status\", StringType(), True), \\\n",
    "#     StructField(\"digest_offset\", StringType(), True), \\\n",
    "#     StructField(\"length\", StringType(), True), \\\n",
    "#     StructField(\"offset\", StringType(), True), \\\n",
    "#     StructField(\"filename\", StringType(), True), \\\n",
    "#     StructField(\"content_charset\", StringType(), True), \\\n",
    "#     StructField(\"language\", StringType(), False)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "607f3881-c421-4b79-98a8-27e945bf4f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 ms, sys: 19 µs, total: 4.17 ms\n",
      "Wall time: 983 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## infer schema for the column header and data type\n",
    "## currently string type for all columns only\n",
    "#    .schema(schema_)\\\n",
    "\n",
    "df_new= spark.read.format(\"parquet\")\\\n",
    "    .load(\"data_sources/aws_crawl_00005/part-00000-4878556a-7ef5-4524-b862-7535a49b03be-c000.snappy.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090de61e-a440-4a93-93f0-124b796e3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new= df_new\\\n",
    "    .withColumn(\"c1_\", F.col(\"_c0\").cast(StringType())).drop(\"_c0\")\\\n",
    "    .withColumn(\"c2_\", F.col(\"_c1\").cast(StringType())).drop(\"_c1\")\\\n",
    "    .withColumn(\"url\", F.col(\"_c2\").cast(StringType())).drop(\"_c2\")\\\n",
    "    .withColumn(\"mime\", F.col(\"_c3\").cast(StringType())).drop(\"_c3\")\\\n",
    "    .withColumn(\"mime_type\", F.col(\"_c4\").cast(StringType())).drop(\"_c4\")\\\n",
    "    .withColumn(\"status\", F.col(\"_c5\").cast(StringType())).drop(\"_c5\")\\\n",
    "    .withColumn(\"digest_offset\", F.col(\"_c6\").cast(StringType())).drop(\"_c6\")\\\n",
    "    .withColumn(\"length\", F.col(\"_c7\").cast(StringType())).drop(\"_c7\")\\\n",
    "    .withColumn(\"offset\", F.col(\"_c8\").cast(StringType())).drop(\"_c8\")\\\n",
    "    .withColumn(\"filename\", F.col(\"_c9\").cast(StringType())).drop(\"_c9\")\\\n",
    "    .withColumn(\"content_charset\", F.col(\"_c10\").cast(StringType())).drop(\"_c10\")\\\n",
    "    .withColumn(\"language\", F.col(\"_c11\").cast(StringType())).drop(\"_c11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e094808-0537-4311-9b29-bf3a823b4bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c1_: string (nullable = true)\n",
      " |-- c2_: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- mime: string (nullable = true)\n",
      " |-- mime_type: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- digest_offset: string (nullable = true)\n",
      " |-- length: string (nullable = true)\n",
      " |-- offset: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- content_charset: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa64478d-15af-40fd-ba43-b4fb21b71a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+\n",
      "|c1_|c2_|                 url|                mime|           mime_type|          status|       digest_offset|             length|              offset|            filename|    content_charset|            language|\n",
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+\n",
      "| au|com|syc)/syc-adopt-pa...| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"200\"| \"digest\": \"3V3JO...|  \"length\": \"19362\"| \"offset\": \"63222...| \"filename\": \"cra...| \"charset\": \"UTF-8\"| \"languages\": \"eng\"}|\n",
      "| au|com|terryflynnbooks)/...| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"200\"| \"digest\": \"SGOZL...| \"length\": \"170686\"| \"offset\": \"11253...| \"filename\": \"cra...| \"charset\": \"UTF-8\"| \"languages\": \"eng\"}|\n",
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65362f1c-4548-4d6b-83e6-dc4eb4d1e087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4051896"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6513d8fc-89dd-49ff-950a-bb344faabfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23783c66-6f19-46f4-867a-d9334bda58d1",
   "metadata": {},
   "source": [
    "### User-Defined Function\n",
    "- Register the `udf` to use with column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d994fa3c-ba24-4561-8e83-5480fd32de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, Split by space\n",
    "first_split= F.split(df_new[\"url\"], \" \")\n",
    "df_new = df_new.withColumn(\"ip_and_resource\", first_split.getItem(0))\\\n",
    "        .withColumn(\"time_stamp\", first_split.getItem(1))\\\n",
    "        .withColumn(\"URL\", first_split.getItem(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f7c1db0-3059-47e7-bfb5-9a507fcb8d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- c1_: string (nullable = true)\n",
      " |-- c2_: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- mime: string (nullable = true)\n",
      " |-- mime_type: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- digest_offset: string (nullable = true)\n",
      " |-- length: string (nullable = true)\n",
      " |-- offset: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- content_charset: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- ip_and_resource: string (nullable = true)\n",
      " |-- time_stamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cedbf649-a0a4-4fc4-aaa7-b0c99bded6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------+\n",
      "|c1_|c2_|                 URL|                mime|           mime_type|          status|       digest_offset|             length|              offset|            filename|    content_charset|            language|     ip_and_resource|    time_stamp|\n",
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------+\n",
      "| au|com|\"https://syc.com....| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"200\"| \"digest\": \"3V3JO...|  \"length\": \"19362\"| \"offset\": \"63222...| \"filename\": \"cra...| \"charset\": \"UTF-8\"| \"languages\": \"eng\"}|syc)/syc-adopt-pa...|20231204203310|\n",
      "| au|com|\"https://www.terr...| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"200\"| \"digest\": \"SGOZL...| \"length\": \"170686\"| \"offset\": \"11253...| \"filename\": \"cra...| \"charset\": \"UTF-8\"| \"languages\": \"eng\"}|terryflynnbooks)/...|20231202152921|\n",
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b4964-d0c5-4f0a-bebe-cc243017a5e5",
   "metadata": {},
   "source": [
    "#### Second split; on ip and resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08d7d317-9e4b-4afa-ac15-795eef27040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col= F.split(df_new['ip_and_resource'], \"\\)\")\n",
    "df_new = df_new.withColumn('ip_path', split_col.getItem(0))\\\n",
    "            .withColumn('resource', split_col.getItem(1))\n",
    "\n",
    "# convert the time stamp\n",
    "#df_new= df_new.withColumn('DateTime', F.to_timestamp(F.col('time_stamp'), \"yyyyMMddHHmmss\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7635efe-bd2b-48d0-83a4-17c11618d214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------+---------------+--------------------+\n",
      "|c1_|c2_|                 URL|                mime|           mime_type|          status|       digest_offset|             length|              offset|            filename|    content_charset|            language|     ip_and_resource|    time_stamp|        ip_path|            resource|\n",
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------+---------------+--------------------+\n",
      "| au|com|\"https://syc.com....| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"200\"| \"digest\": \"3V3JO...|  \"length\": \"19362\"| \"offset\": \"63222...| \"filename\": \"cra...| \"charset\": \"UTF-8\"| \"languages\": \"eng\"}|syc)/syc-adopt-pa...|20231204203310|            syc|/syc-adopt-pacer-...|\n",
      "| au|com|\"https://www.terr...| \"mime\": \"text/html\"| \"mime-detected\":...| \"status\": \"200\"| \"digest\": \"SGOZL...| \"length\": \"170686\"| \"offset\": \"11253...| \"filename\": \"cra...| \"charset\": \"UTF-8\"| \"languages\": \"eng\"}|terryflynnbooks)/...|20231202152921|terryflynnbooks|  /more-book-reviews|\n",
      "+---+---+--------------------+--------------------+--------------------+----------------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------+---------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Drop the columns\n",
    "#df_new= df_new.drop(*['ip_and_resource', 'time_stamp'])\n",
    "df_new.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f7b4ca5-8516-45e9-a28b-27789d02de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69eb4b-539f-4bff-b3de-fec6c6973913",
   "metadata": {},
   "source": [
    "#### NOTE: Saves files in different folder as the data has been paritioned\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca960c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35b35d89-c807-4d9d-ba8d-c2649254fbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x7df45acb2650>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_new.write.csv('data_sources/aws_crawl_00005/first_part_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca58805c-a104-4ee2-a25a-dcb45f4c3d70",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o384.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: data_sources/aws_crawl_00005/first_par_csv. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: data_sources/aws_crawl_00005/first_par_csv.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## using coalase\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[43mdf_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_sources/aws_crawl_00005/first_par_csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfirst_.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o384.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: data_sources/aws_crawl_00005/first_par_csv. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.ClassNotFoundException: data_sources/aws_crawl_00005/first_par_csv.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "## using coalase\n",
    "\n",
    "df_new.repartition(1)\\\n",
    "    .write.format(\"data_sources/aws_crawl_00005/first_par_csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .save(\"first_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1081b18d-93b0-49c8-9617-8bec6c7780bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "718a43c5-13cb-4270-82ce-41f08c3d44ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 16 fields in line 3803, saw 17\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 16 fields in line 3803, saw 17\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_parq= pd.read_csv(\"data_sources/aws_crawl_00005/first_part_csv/part-00004-9c5d555c-6feb-4672-a481-a67c0bf94d0a-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f456ff-a8aa-4417-919e-116aa8a8119b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
